{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rapidfuzz\n",
    "from rapidfuzz.process import extract, extract_iter , extractOne\n",
    "import fuzzywuzzy\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapi = pd.read_csv(\"mapi_list_sn_ln.csv\")\n",
    "mapi.iloc[3752][0] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "73\n",
    "answers = ['No validated clinical questionnaires were used in this text.',\n",
    " 'No validated clinical questionnaires were used in this text.',\n",
    " 'No validated clinical questionnaires were used in this text.',\n",
    " 'No validated clinical questionnaires were used in this text.',\n",
    " 'No validated clinical questionnaires were used in this text.',\n",
    " 'No validated clinical questionnaires were used in this text.',\n",
    " 'No validated clinical questionnaires were used in this text.',\n",
    " 'No validated clinical questionnaires were used in this text.',\n",
    " 'No validated clinical questionnaires were used in this text.',\n",
    " 'No validated clinical questionnaires were used in this text.',\n",
    " 'No validated clinical questionnaires were used in this text.',\n",
    " '- Fullerton Advanced Balance Scale\\n- Falls Efficacy Scale\\n- SF-36\\n- Geriatric Depression Scale\\n- Geriatric Anxiety Scale\\n- Apathy Evaluation Scale\\n- Expanded Short Physical Performance Battery\\n- Zarit Caregiver Burden Scale\\n- Neuropsychiatric Inventory Questionnaire\\n- Alternative Uses Task\\n- Postural Sway\\n- Gait Speed and Variability\\n- Timed Up and Go\\n- Philadelphia Mindfulness Scale\\n- Abbreviated Cognitive Battery\\n- Multisensory Reaction Time\\n- MRI scan of brain (not a questionnaire)\\n- IMOVE Protocol, Version 6.0 (not a questionnaire)',\n",
    " 'No validated clinical questionnaires were used in this text.',\n",
    " 'No validated clinical questionnaires were used in this text.',\n",
    " 'No validated clinical questionnaires were used in this text.',\n",
    " 'No validated clinical questionnaires were used in this text.',\n",
    " 'No validated clinical questionnaires were used in this text.',\n",
    " 'No validated clinical questionnaires were used in this text.',\n",
    " 'No validated clinical questionnaires were used in this text.',\n",
    " 'No validated clinical questionnaires were used in this text.',\n",
    " 'No validated clinical questionnaires were used in this text.',\n",
    " 'No validated clinical questionnaires were used in this text.',\n",
    " 'No validated clinical questionnaires were used in this text.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 69\n",
    "answers = ['- FACT-G (Version 4)\\n- EQ-5D-5L Questionnaire',\n",
    " 'No validated clinical questionnaires were used in this text.',\n",
    " 'No validated clinical questionnaires were used in this text.',\n",
    " '- EQ-5D-5L, EuroQol group\\n- FACT-G, Functional Assessment of Cancer Therapy-General',\n",
    " 'No validated clinical questionnaires were used in this text.',\n",
    " 'No validated clinical questionnaires were used in this text.',\n",
    " 'No validated clinical questionnaires were used in this text.',\n",
    " 'No validated clinical questionnaires were used in this text.',\n",
    " 'No validated clinical questionnaires were used in this text.',\n",
    " 'No validated clinical questionnaires were used in this text.',\n",
    " 'No validated clinical questionnaires were used in this text.',\n",
    " 'No validated clinical questionnaires were used in this text.',\n",
    " 'No validated clinical questionnaires were used in this text.',\n",
    " 'No validated clinical questionnaires were used in this text.',\n",
    " 'No validated clinical questionnaires were used in this text.',\n",
    " 'No validated clinical questionnaires were used in this text.',\n",
    " 'No validated clinical questionnaires were used in this text.',\n",
    " 'No validated clinical questionnaires were used in this text.',\n",
    " 'No validated clinical questionnaires were used in this text.',\n",
    " 'No validated clinical questionnaires were used in this text.',\n",
    " 'No validated clinical questionnaires were used in this text.',\n",
    " 'No validated clinical questionnaires were used in this text.',\n",
    " 'No validated clinical questionnaires were used in this text.',\n",
    " 'No validated clinical questionnaires were used in this text.',\n",
    " 'No validated clinical questionnaires were used in this text.',\n",
    " '- FACT-G (Version 4)\\n- EQ-5D-5L Questionnaire',\n",
    " '- EQ-5D-5L questionnaire\\n- FACT-G questionnaire']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def key(tup):\n",
    "    return tup[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_cleaning(my_list):\n",
    "    temp_list1 = []\n",
    "    temp_list2 = []\n",
    "    for x in my_list:\n",
    "        # if not x == 'Not found.':\n",
    "        if not x == 'No validated clinical questionnaires were used in this text.':\n",
    "            temp_list1.append(x)\n",
    "    for y in temp_list1:\n",
    "        split = y.split('\\n')\n",
    "        for s in split:\n",
    "            temp_list2.append(s)\n",
    "    return temp_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_iter_long(string, long=True):\n",
    "    matches = []\n",
    "    choices = mapi.long_name\n",
    "    if long == False:\n",
    "        choices = mapi.short_name\n",
    "    cut_off = 0.7\n",
    "\n",
    "    for choice in choices:\n",
    "        score = rapidfuzz.distance.Levenshtein.normalized_similarity(string, choice, processor=rapidfuzz.utils.default_process, weights=(1,0.999999,1))\n",
    "        if score >= cut_off and \"...\" not in choice:\n",
    "            matches.append((choice, score))\n",
    "        \n",
    "    matches = sorted(matches, key=key, reverse=True)\n",
    "    \n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join algorithms to match name\n",
    "def compound(string, long=True):\n",
    "\n",
    "    weighted = weighted_iter_long(string,long)\n",
    "    ratio = []\n",
    "    for answer in weighted:\n",
    "        y = rapidfuzz.fuzz.partial_ratio(string, answer[0])/100\n",
    "        score = answer[1] + y\n",
    "        ratio.append((answer[0], score))\n",
    "    actual = []\n",
    "    for answer in ratio:\n",
    "        x = rapidfuzz.distance.Levenshtein.normalized_similarity(string, answer[0], weights=(1,1,1))\n",
    "        score = answer[1] + x\n",
    "        actual.append((answer[0], score))\n",
    "    \n",
    "    actual = sorted(actual, key=key, reverse=True)\n",
    "    cutoff = 1.6\n",
    "    if len(actual) > 1:\n",
    "        if actual[1][1] >= cutoff:\n",
    "            actual = actual[0]\n",
    "        else:\n",
    "            actual = ()\n",
    "    elif len(actual) == 1:\n",
    "        if actual[0][1] >= cutoff:\n",
    "            actual = actual[0]\n",
    "        else:\n",
    "            actual = ()\n",
    "    else:\n",
    "        actual = ()\n",
    "    return actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# short + long names \n",
    "def joint(string):\n",
    "    short = compound(string, long=False)\n",
    "    long  = compound(string)\n",
    "    best = (long, 'l')\n",
    "    if short and not long:\n",
    "        best = (short,'s')\n",
    "    if short and long:\n",
    "        if short[1] > long[1]:\n",
    "            best = (short,'s')\n",
    "    if best[0]:\n",
    "        best = best[0][0], best[1]\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_cleaning_answers(my_list):\n",
    "    new = list(filter(None, my_list))\n",
    "    almost = {}\n",
    "    short = []\n",
    "    long = []\n",
    "    for q in new:\n",
    "        if q[1] == 's':\n",
    "            short.append(q[0])\n",
    "        else:\n",
    "            long.append(q[0])\n",
    "    short = list(filter(None, short))\n",
    "    long = list(filter(None, long))\n",
    "    for q in short:\n",
    "        idx = mapi.index[mapi['short_name'] == q]\n",
    "        if mapi.iloc[idx[0]][0] not in almost:\n",
    "            almost[mapi.iloc[idx[0]][0]] = mapi.iloc[idx[0]][1]\n",
    "\n",
    "    for q in long:\n",
    "        idx = mapi.index[mapi['long_name'] == q]\n",
    "        if mapi.iloc[idx[0]][0] not in almost:\n",
    "            almost[mapi.iloc[idx[0]][0]] = mapi.iloc[idx[0]][1]\n",
    "\n",
    "\n",
    "    return almost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_cleaning(my_list):\n",
    "    temp_list = []\n",
    "    my_list = list(filter(None, my_list))\n",
    "    for token in my_list:\n",
    "        temp_list.append(rapidfuzz.utils.default_process(token))\n",
    "        temp_list.append(token)\n",
    "    temp_list = list(dict.fromkeys(my_list))\n",
    "\n",
    "    return temp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_answers = answer_cleaning(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['- FACT-G (Version 4)',\n",
       " '- EQ-5D-5L Questionnaire',\n",
       " '- EQ-5D-5L, EuroQol group',\n",
       " '- FACT-G, Functional Assessment of Cancer Therapy-General',\n",
       " '- FACT-G (Version 4)',\n",
       " '- EQ-5D-5L Questionnaire',\n",
       " '- EQ-5D-5L questionnaire',\n",
       " '- FACT-G questionnaire']"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "for answer in cleaned_answers:\n",
    "    x.append(joint(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('FACT-G', 's'),\n",
       " ('EAR-Questionnaire', 'l'),\n",
       " ('EQ-5D-5L', 's'),\n",
       " ('Functional Assessment of Cancer Therapy - General', 'l'),\n",
       " ('FACT-G', 's'),\n",
       " ('EAR-Questionnaire', 'l'),\n",
       " ('EQ-5D-5L', 's'),\n",
       " ('ChAt questionnaire', 's')]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pre_cleaning_answers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FACT-G': 'Functional Assessment of Cancer Therapy - General',\n",
       " 'EQ-5D-5L': 'EuroQoL 5-Dimension 5-Level',\n",
       " 'ChAt questionnaire': 'Children Atopy questionnaire',\n",
       " 'EAR-Q': 'EAR-Questionnaire'}"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
