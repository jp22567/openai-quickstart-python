{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF to Questionnaires Pipeline Demo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section Cutting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader, PdfWriter\n",
    "import pandas as pd\n",
    "import os\n",
    "import fitz\n",
    "from re import search\n",
    "import openai\n",
    "from transformers import GPT2TokenizerFast\n",
    "import rapidfuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for finding \"relevant\" pages where the questionnaires could occur.\n",
    "def page_finder(protocol):\n",
    "    # list of potential keywords to find relevant sections\n",
    "    potential_sections = [\n",
    "    \"objective\",\n",
    "    \"study endpoints\",\n",
    "    \"event\",\n",
    "    \"assessments\",\n",
    "    \"activities\",\n",
    "    \"abbreviations\",\n",
    "    \"endpoint\",\n",
    "    \"evaluation\",\n",
    "    \"measure\",\n",
    "    \"design\",\n",
    "    \"synopsis\",\n",
    "    \"questionnaire\",\n",
    "    \"outcome\",\n",
    "    \"patient reported\",\n",
    "    \"definitions\",\n",
    "    \"flow chart\",\n",
    "    \"visits\",\n",
    "    \"schedule\",\n",
    "    ]\n",
    "    # table of contents\n",
    "    # input_pdf = PdfReader(open(protocol, 'rb'))\n",
    "    try:\n",
    "        pdf = fitz.open(protocol)\n",
    "        toc = pdf.get_toc()\n",
    "        # header = page number\n",
    "        section_titles = {}\n",
    "        for item in toc:\n",
    "            section_titles[item[1]] = item[2]\n",
    "        # page numbers to extract\n",
    "        page_nos = []\n",
    "        for title in section_titles.keys():\n",
    "            for potential in potential_sections:\n",
    "                if search(potential, title.lower()):\n",
    "                    page_nos.append(section_titles[title])\n",
    "        page_nos = list(dict.fromkeys(page_nos))\n",
    "        page_nos2 = page_nos.copy()\n",
    "        for page in page_nos:\n",
    "            page_nos2.append(page + 1)\n",
    "        page_nos2 = list(dict.fromkeys(page_nos2))\n",
    "        # try:\n",
    "        #     if page_nos2[-1] >= len(input_pdf.pages):\n",
    "        #         page_nos2.remove(page_nos2[-1])\n",
    "        # except:\n",
    "        #     print(protocol, \"finder\")\n",
    "        return page_nos2\n",
    "    except:\n",
    "        print(protocol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take page numbers from page_finder and cut those pages out making a new pdf at the specified path\n",
    "def pdf_writer(inpath,  outpath, page_nos):\n",
    "    input_pdf = PdfReader(open(inpath,'rb'))\n",
    "    output_pdf = PdfWriter()\n",
    "    for i in page_nos:\n",
    "        page = input_pdf.pages[i]\n",
    "        output_pdf.add_page(page)\n",
    "    with open(outpath, 'wb') as f:\n",
    "        output_pdf.write(f)\n",
    "        # ok_count += 1\n",
    "\n",
    "        print(\"ok\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section Cutting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Page numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_nos = page_finder(r\"C:\\Users\\Jakub\\Documents\\zazu\\openai-quickstart-python\\protocols\\clinical_trial_rank_0005.pdf\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cut PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'C:\\\\Users\\\\Jakub\\\\Documents\\\\zazu\\\\openai-quickstart-python\\\\demo_app\\\\cut_trial_rank_0005.pdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m pdf_writer(\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mC:\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mUsers\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mJakub\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mDocuments\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mzazu\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mopenai-quickstart-python\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mprotocols\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mclinical_trial_rank_0005.pdf\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      2\u001b[0m             \u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mC:\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mUsers\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mJakub\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mDocuments\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mzazu\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mopenai-quickstart-python\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mdemo_app\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mcut_trial_rank_0005.pdf\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      3\u001b[0m               page_nos)\n",
      "Cell \u001b[1;32mIn[3], line 8\u001b[0m, in \u001b[0;36mpdf_writer\u001b[1;34m(inpath, outpath, page_nos)\u001b[0m\n\u001b[0;32m      6\u001b[0m     page \u001b[39m=\u001b[39m input_pdf\u001b[39m.\u001b[39mpages[i]\n\u001b[0;32m      7\u001b[0m     output_pdf\u001b[39m.\u001b[39madd_page(page)\n\u001b[1;32m----> 8\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(outpath, \u001b[39m'\u001b[39;49m\u001b[39mwb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m      9\u001b[0m     output_pdf\u001b[39m.\u001b[39mwrite(f)\n\u001b[0;32m     10\u001b[0m     \u001b[39m# ok_count += 1\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jakub\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[0;32m    276\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    277\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    279\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m     )\n\u001b[1;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'C:\\\\Users\\\\Jakub\\\\Documents\\\\zazu\\\\openai-quickstart-python\\\\demo_app\\\\cut_trial_rank_0005.pdf'"
     ]
    }
   ],
   "source": [
    "pdf_writer(r\"C:\\Users\\Jakub\\Documents\\zazu\\openai-quickstart-python\\protocols\\clinical_trial_rank_0005.pdf\",\n",
    "            r\"C:\\Users\\Jakub\\Documents\\zazu\\openai-quickstart-python\\demo_app\\cut_trial_rank_0005.pdf\",\n",
    "              page_nos)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questionnaire Extraction functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.organization = \"org-us16wmNswbfs7htSVY2eiaYh\" # zazu\n",
    "openai.api_key = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cut text into chunks small enough for the GPT API\n",
    "def chunker(text):\n",
    "    tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "    chunks = []\n",
    "    page = 0\n",
    "    while page < len(text):\n",
    "        page_count = 0\n",
    "        chunk_tokens = 0\n",
    "        chunk = \"\"\n",
    "        # add pages until the page or token limit is reached\n",
    "        while chunk_tokens <= 3500 and page_count < 5 and page < len(text):\n",
    "            chunk += text.iloc[page][2]\n",
    "            chunk_tokens += len(tokenizer(chunk)[0])\n",
    "            page += 1\n",
    "            page_count += 1\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes list of chunks and runs them through the GPT API, returns list of answers, one for each chunk\n",
    "def gpt(chunks):\n",
    "    answers = []\n",
    "    for chunk in chunks:\n",
    "        completion = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "                {\"role\": \"system\", \"content\": \"\"\"You are a helpful assistant that extracts validated clinical questionnaires and puts them into comma separated list with each questionnaire on a new line. Do not ask questions.\n",
    "                If you can't find any questionnaires say: 'None found'.\n",
    "                example text: nation', 'EMA', 'European Medicines Agency', 'DLQI', 'Dermatology Life Quality Index', 'EQ-5D', 'European Quality of Life 5-Dimension Questionnaire', 'FCBP', 'Females of childbearing potential', 'FDA', 'Food and Drug Administration'\n",
    "                example answer: \n",
    "                - EQ-5D, European Quality of Life 5-Dimension Questionnaire\n",
    "                - DLQI, Dermatology Life Quality Index\"\"\"},\n",
    "                {\"role\": \"user\", \"content\": chunk},\n",
    "            ]\n",
    "        )\n",
    "        answer = completion[\"choices\"][0][\"message\"][\"content\"]\n",
    "        answers.append(answer)\n",
    "\n",
    "    return answers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questionnaire Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"C:\\Users\\Jakub\\Documents\\zazu/openai-quickstart-python/text/clinical_trial_rank_0005/text.csv\")\n",
    "text = data.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1214 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "chunks = chunker(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = gpt(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None found.\n",
      "None found.\n",
      "- FACT-G, Functional Assessment of Cancer Therapy-General\n",
      "None found.\n",
      "None found.\n",
      "None found.\n",
      "None found.\n",
      "None found.\n",
      "None found.\n",
      "None found.\n",
      "None found.\n",
      "None found.\n",
      "None found.\n",
      "None found.\n",
      "- FACT-G, FACT-G QUESTIONNAIRE, VERSION 4, ENGLISH AND SWAHILI\n",
      "None found.\n"
     ]
    }
   ],
   "source": [
    "for answer in answers:\n",
    "    print(answer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "GPT Answers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- None found.\n",
    "- FACT-G, Functional Assessment of Cancer Therapy-General\n",
    "- None found.\n",
    "- FACT-G, FACT-G QUESTIONNAIRE, VERSION 4, ENGLISH AND SWAHILI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuzzy Matching Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv of questionnaires\n",
    "mapi = pd.read_csv(r\"C:\\Users\\Jakub\\Documents\\zazu\\openai-quickstart-python\\timings\\mapi_list_sn_ln.csv\")\n",
    "mapi.iloc[3752][0] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for sorting\n",
    "def key(tup):\n",
    "    return tup[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the answers before trying to match\n",
    "def answer_cleaning(my_list):\n",
    "    temp_list1 = []\n",
    "    temp_list2 = []\n",
    "    temp_list3 = []\n",
    "    temp_list4 = []\n",
    "    for x in my_list:\n",
    "        if not x == 'None found.':\n",
    "            # if not x == 'No validated clinical questionnaires were used in this text.':\n",
    "            temp_list1.append(x)\n",
    "    for y in temp_list1:\n",
    "        split = y.split('\\n')\n",
    "        for s in split:\n",
    "            temp_list2.append(s)\n",
    "    for z in temp_list2:\n",
    "        if not 'not a questionnaire' in z:\n",
    "            temp_list3.append(z)\n",
    "    for string in temp_list3:\n",
    "        tokens = string.split()\n",
    "        # word 'questionnaire' skews answers so remove it\n",
    "        tokens = [token if token != 'questionnaire' else '' for token in tokens]\n",
    "        tokens = [token if token != 'Questionnaire' else '' for token in tokens]\n",
    "        string = ' '.join(tokens)\n",
    "        temp_list4.append(string)\n",
    "    return temp_list4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my version of the rapidfuzz.process.extract_iter() which allows for change of weights of the processor\n",
    "# compares the string with all the long names in the csv above\n",
    "def weighted_iter_long(string):\n",
    "    matches = []\n",
    "    choices = mapi.long_name\n",
    "    cut_off = 0.7\n",
    "\n",
    "    for choice in choices:\n",
    "        score = rapidfuzz.distance.Levenshtein.normalized_similarity(string, choice, processor=rapidfuzz.utils.default_process, weights=(0.999999,1,1))\n",
    "        # remove incomplete answers where '...' occurs\n",
    "        if score >= cut_off and \"...\" not in choice:\n",
    "            matches.append((choice, score))\n",
    "    \n",
    "    # sort the list in descending order\n",
    "    matches = sorted(matches, key=key, reverse=True)\n",
    "    \n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as above but for the short names\n",
    "# splits the string to compare the individual tokens to the short names\n",
    "def weighted_iter_short(string):\n",
    "    matches = []\n",
    "    tokens = string.split()\n",
    "    tokens = [token if token != 'questionnaire' else '' for token in tokens]\n",
    "    choices = mapi.short_name\n",
    "    cut_off = 0.7\n",
    "    for token in tokens:\n",
    "        for choice in choices:\n",
    "            score = rapidfuzz.distance.Levenshtein.normalized_similarity(token, choice, processor=rapidfuzz.utils.default_process, weights=(1,0.999999,1))\n",
    "            if score >= cut_off and \"...\" not in choice:\n",
    "                matches.append((choice, score))\n",
    "        \n",
    "    matches = sorted(matches, key=key, reverse=True)\n",
    "    \n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join three algorithms to match long name\n",
    "# all normalized scores are added together to find the closest match\n",
    "# cutoffs can be changed to increase or the decrease number of potential answers but only the top answer is outputted \n",
    "def long_compound(string, long=True):\n",
    "\n",
    "    weighted = weighted_iter_long(string)\n",
    "    ratio = []\n",
    "    for answer in weighted:\n",
    "        y = rapidfuzz.fuzz.partial_ratio(string, answer[0])/100\n",
    "        score = answer[1] + y\n",
    "        ratio.append((answer[0], score))\n",
    "    actual = []\n",
    "    for answer in ratio:\n",
    "        x = rapidfuzz.distance.Levenshtein.normalized_similarity(string, answer[0], weights=(1,1,1))\n",
    "        score = answer[1] + x\n",
    "        actual.append((answer[0], score))\n",
    "    \n",
    "    actual = sorted(actual, key=key, reverse=True)\n",
    "    cutoff = 1.6\n",
    "    if len(actual) > 1:\n",
    "        if actual[1][1] >= cutoff:\n",
    "            actual = actual[0]\n",
    "        else:\n",
    "            actual = ()\n",
    "    elif len(actual) == 1:\n",
    "        if actual[0][1] >= cutoff:\n",
    "            actual = actual[0]\n",
    "        else:\n",
    "            actual = ()\n",
    "    else:\n",
    "        actual = ()\n",
    "    return actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as above but for short names\n",
    "def short_compound(string):\n",
    "    tokens = string.split()\n",
    "    weighted = weighted_iter_short(string)\n",
    "    ratio = []\n",
    "    for answer in weighted:\n",
    "        for token in tokens:\n",
    "            y = rapidfuzz.fuzz.partial_ratio(token, answer[0])/100\n",
    "            score = answer[1] + y\n",
    "            ratio.append((answer[0], score))\n",
    "    actual = []\n",
    "    for answer in ratio:\n",
    "        for token in tokens:\n",
    "            x = rapidfuzz.distance.Levenshtein.normalized_similarity(string, answer[0], weights=(1,1,1))\n",
    "            score = answer[1] + x\n",
    "            actual.append((answer[0], score))\n",
    "\n",
    "    actual = list(filter(None, actual))\n",
    "    actual = list(dict.fromkeys(actual))\n",
    "    actual = sorted(actual, key=key, reverse=True)\n",
    "    \n",
    "    cutoff = 2\n",
    "    if len(actual) > 1:\n",
    "        if actual[1][1] >= cutoff:\n",
    "            actual = actual[0]\n",
    "        else:\n",
    "            actual = ()\n",
    "    elif len(actual) == 1:\n",
    "        if actual[0][1] >= cutoff:\n",
    "            actual = actual[0]\n",
    "        else:\n",
    "            actual = ()\n",
    "    else:\n",
    "        actual = ()\n",
    "    return actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the short + long name functions, taking the best score \n",
    "# s stands for short name and l for long name\n",
    "def joint(string):\n",
    "    # print(string)\n",
    "    short = short_compound(string)\n",
    "    long  = long_compound(string)\n",
    "    # print(string, short , long)\n",
    "    best = (long, 'l')\n",
    "    if short and not long:\n",
    "        best = (short,'s')\n",
    "    if short and long:\n",
    "        if short[1] > long[1]:\n",
    "            best = (short,'s')\n",
    "    if best[0]:\n",
    "        best = best[0][0], best[1]\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleans the questionnaires and matches long and short names, outputting a dictionary\n",
    "def questionnaire_output(my_list):\n",
    "    new = list(filter(None, my_list))\n",
    "    almost = {}\n",
    "    short = []\n",
    "    long = []\n",
    "    for q in new:\n",
    "        if q[1] == 's':\n",
    "            short.append(q[0])\n",
    "        else:\n",
    "            long.append(q[0])\n",
    "    short = list(filter(None, short))\n",
    "    long = list(filter(None, long))\n",
    "    for q in short:\n",
    "        idx = mapi.index[mapi['short_name'] == q]\n",
    "        if mapi.iloc[idx[0]][0] not in almost:\n",
    "            almost[mapi.iloc[idx[0]][0]] = mapi.iloc[idx[0]][1]\n",
    "\n",
    "    for q in long:\n",
    "        idx = mapi.index[mapi['long_name'] == q]\n",
    "        if mapi.iloc[idx[0]][0] not in almost:\n",
    "            almost[mapi.iloc[idx[0]][0]] = mapi.iloc[idx[0]][1]\n",
    "\n",
    "\n",
    "    return almost"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuzzy Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_answers = answer_cleaning(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_answers = answer_cleaning(cleaned_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questionnaires = []\n",
    "for answer in cleaned_answers:\n",
    "    questionnaires.append(joint(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Functional Assessment of Cancer Therapy - General', 'l'), ('FACT-G', 's')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questionnaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = questionnaire_output(questionnaires)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FACT-G': 'Functional Assessment of Cancer Therapy - General'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final List\n",
    "{'FACT-G': 'Functional Assessment of Cancer Therapy - General'}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timings - To be done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "12d4f619691cb3c8cdb4fc1bac3542d3bd06428ac0d8596b7ce847b27c5c0d5e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
